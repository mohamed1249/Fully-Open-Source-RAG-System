{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a9141e0b-b8fd-4b7e-9acd-78ea1a1b8280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\python\\projects\\langchain\\fun_agent_env\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\python\\projects\\langchain\\fun_agent_env\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\python\\projects\\langchain\\fun_agent_env\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-ollama\n",
    "%pip install -qU langchain-community\n",
    "%pip install -qU sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "738fbffb-5507-4f39-808e-7ebb6bf776e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_ollama'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_ollama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOllama\n\u001b[0;32m      3\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOllama(\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m,\n\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_ollama'"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3\",\n",
    "    temperature=0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1d0695e8-7591-4c6b-8648-6129e4526650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations on your upcoming graduation! I'd be happy to help you come up with some ideas for your graduation project, especially if it's related to Artificial Intelligence (AI). Here are a few potential concepts to get you started:\n",
      "\n",
      "1. **Chatbot for Customer Service**: Design and develop a chatbot that can assist customers with common queries or issues. You could integrate this with a popular platform like Facebook Messenger or WhatsApp.\n",
      "2. **Image Classification using Deep Learning**: Create a project that uses convolutional neural networks (CNNs) to classify images into different categories, such as objects, scenes, or emotions.\n",
      "3. **Natural Language Processing (NLP) for Sentiment Analysis**: Develop an NLP-based system that can analyze text data and determine the sentiment (positive, negative, neutral) of a given piece of writing.\n",
      "4. **Recommendation System using Collaborative Filtering**: Design a recommendation engine that suggests products or services based on user behavior and preferences.\n",
      "5. **AI-Powered Music Generation**: Use generative models like Generative Adversarial Networks (GANs) to create music or generate musical patterns.\n",
      "6. **Object Detection using YOLO or SSD**: Implement an object detection algorithm, such as You Only Look Once (YOLO) or Single Shot Detector (SSD), to identify objects in images or videos.\n",
      "7. **Speech Recognition and Synthesis**: Develop a speech recognition system that can transcribe spoken language into text, and also create a speech synthesis system that can generate human-like voices.\n",
      "8. **Game Development using AI**: Create a game that incorporates AI-powered gameplay mechanics, such as adaptive difficulty levels, AI-controlled opponents, or predictive analytics for player behavior.\n",
      "9. **Predictive Maintenance using Machine Learning**: Develop a system that uses machine learning algorithms to predict equipment failures and optimize maintenance schedules.\n",
      "10. **AI-Powered Virtual Assistants**: Design an AI-powered virtual assistant that can perform tasks, answer questions, and provide recommendations based on user inputs.\n",
      "\n",
      "These ideas should give you a good starting point for your graduation project. Remember to choose something that interests you and aligns with your skills and expertise. Good luck!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that answers questions related to {subject}. Answer the user question.\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "answer = chain.invoke(\n",
    "    {\n",
    "        \"subject\": \"AI\",\n",
    "        \"question\": \"Give me Ideas of a graduation project.\",\n",
    "    }\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e6fa60-2a6a-43f7-9ab6-16a282e5a050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Python\\projects\\LangChain\\RAG\n"
     ]
    }
   ],
   "source": [
    "!cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0190fc81-bc3b-46f3-a519-9fed50db65ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain import hub\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a9ba221-652d-444e-8069-de8c83f8cb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (191, 384)\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"Doc.pdf\"\n",
    "loader = PyPDFLoader(file_path=pdf_path)\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=30, separator=\"\\n\"\n",
    ")\n",
    "docs = text_splitter.split_documents(documents=documents)\n",
    "str_docs = []\n",
    "for i,doc in enumerate(docs):\n",
    "    str_docs.append(str(docs[i]))\n",
    "    \n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') # Loading our embedding model.\n",
    "\n",
    "# Generate embeddings for the sentences\n",
    "embeddings = model.encode(str_docs)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Dimensions of our embeddings\n",
    "d = embeddings.shape[1]\n",
    "\n",
    "# Creating an index for our dense vectors\n",
    "index = faiss.IndexFlatL2(d)  # Using L2 (Euclidean) distance\n",
    "\n",
    "# Adding the embeddings to the index\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5344793a-acc4-4b88-b32c-3f375d79fba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache\n",
    "from typing import Iterator, Mapping, TypeVar, cast\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "\n",
    "@cache\n",
    "def _model():\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "807c6c9b-b533-4cae-a7d4-2c6b766ed896",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorMap(Mapping[str, T]):\n",
    "    \"\"\"\n",
    "    A dictionary that uses sentence-transformers to do vector comparison on\n",
    "    keys\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, threshold: float = 0.7):\n",
    "        self._data: list[T] = data\n",
    "        self._model = _model()\n",
    "        self._embeddings = self._model.encode(self._data, convert_to_tensor=True)\n",
    "        self._length = len(data)\n",
    "        self._threshold = threshold\n",
    "\n",
    "    def __getitem__(self, key) -> T:\n",
    "        out = []\n",
    "        top_k = 4\n",
    "        query_embedding = self._model.encode(key, convert_to_tensor=True)\n",
    "        similarity_scores = self._model.similarity(query_embedding, self._embeddings)[0]\n",
    "        scores, indices = torch.topk(similarity_scores, k=top_k)\n",
    "        for score, index in zip(scores, indices):\n",
    "            if score > self._threshold:\n",
    "                out.append(self._data[index])\n",
    "        return out\n",
    "\n",
    "    def __contains__(self, key) -> bool:\n",
    "        query_embedding = self._model.encode(key, convert_to_tensor=True)\n",
    "        scores = self._model.similarity_scores(query_embedding, self._embeddings)\n",
    "        return any(score > self._threshold for score in scores)\n",
    "\n",
    "    def __iter__(self) -> Iterator[str]:\n",
    "        return iter(self._data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"VectorMap()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1266c074-a411-4999-bc65-c20c90136642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['page_content=\\'parameters, offering a balance between performance and efficiency. While smaller than\\nGPT-3.5 and the rumored size of GPT-4, it still demonstrates impressive capabilities.\\nOpen-Sour ce: Released under an open-source license, Llama v2 allows anyone to access,\\nstudy, and modify the underlying code. This fosters collaboration and innovation in the\\nfield of LLMs.\\nFocus on Safety:  Meta emphasizes safety and factual accuracy with Llama v2. This makes it\\na potentially strong candidate for applications where trust and responsible AI are crucial,\\nsuch as mental health chatbots or educational tools.\\nIn [ ]:while True:\\n    message = input()\\n    if message == \\'Done\\':\\n        break\\n    print(f\"User Message: {message}\")\\n    print(f\"LLM Responce: {chain.run(message)}\") # This is an Arabic test for GPT-4\\' metadata={\\'source\\': \\'Doc.pdf\\', \\'page\\': 22}', \"page_content='wasn't trained on. A continuously decreasing loss on validation data indicates the model is\\nlearning effectively.\\nEvaluat e task -specific metr ics: Use metrics relevant to your specific task. For example,\\naccuracy for classification or F1 score for question answering. These give a clearer picture of\\nthe model's generalizability.\\ndirectly t est y our LLM' s responces : since the LLM we're using is meant to be a\\nconversational AI chatbot, a great way to test its performance would be to directly interact' metadata={'source': 'Doc.pdf', 'page': 58}\", \"page_content='You are a Mental Health Professional who helps people with mental disorders like auti\\nsm and ADHD.\\nUsing the contexts and chat history below, answer the query.\\n    Contexts:\\n    {'information': 'Inadequate or variable self-app lication to tasks that require s\\nustained effort is often in-\\\\nterpreted by others as laziness , irresponsibility, or  \\nfailure to  cooperate. Family relation-\\\\nships may be characterized by discord and ne  \\ngative interactions. Peer relationships are\\\\noften disrupted by peer rejection, negle\\nct, or  teasing of the individual with ADHD. On av-\\\\nerage, individuals with ADHD obt\\nain less schooling, have poorer vocational achievement,\\\\nand have reduced intellectua\\nl scores than their peers, although there is great variability. In\\\\nits severe form,  \\nthe disorder is markedly impairing, affecting social, familial, and scholas-\\\\ntic/occ\\nupational adjustment', 'page_number': 108.0, 'source_file_name': 'Diagnostic and stat' metadata={'source': 'Doc.pdf', 'page': 52}\", \"page_content='Signs and sympt oms:\\nDifficulty with communication and social interaction\\nRestricted interests and repetitive behaviors\\nDiagnosis:\\nDiagnosed by evaluating a person's behavior and development\\nTreatment:\\nEarly intervention and treatment is important, and can include medication,\\nbehavioral, psychological, and educational interventions.\\nPreprocessing Techniques:' metadata={'source': 'Doc.pdf', 'page': 14}\"]\n"
     ]
    }
   ],
   "source": [
    "vector_map = VectorMap(str_docs, threshold=0)\n",
    "\n",
    "print(vector_map[\"This is just a test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae65d9ab-69e7-42a3-807d-2de30a73e43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"page_content='data in a high-dimensional space. Imagine them as unique fingerprints for your data points!\\nHere are some key concepts to grasp embedding models:\\nInput and Output:  They take raw, unprocessed data (think a sentence or an image) as\\ninput and generate a vector of numbers (the embedding) as output.\\nDimensionality:  The number of dimensions in this embedding vector determines the level\\nof detail it can capture. Higher dimensions allow for more nuanced representations, but\\nalso require more computational resources.\\nSimilar ity: The magic lies in how embeddings represent relationships. Similar data points,\\nlike sentences with similar meanings, will have embeddings closer together in the vector\\nspace. Conversely, dissimilar data points will be further apart. This allows for efficient\\nsimilarity searches – finding data points with characteristics close to a given query.\\nPre-trained vs. Fine-tuned:  Embedding models can be:' metadata={'source': 'Doc.pdf', 'page': 39}\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_map[\"What is embeddings\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c1149a11-513f-4491-827a-52cc87c15858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\projects\\langchain\\fun_agent_env\\lib\\site-packages\\langsmith\\client.py:354: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001A5A6024B80>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'langchain-ai', 'lc_hub_repo': 'retrieval-qa-chat', 'lc_hub_commit_hash': 'b60afb6297176b022244feb83066e10ecadcda7b90423654c4a9d45e7a73cebc'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Answer any use questions based solely on the context below:\\n\\n<context>\\n{context}\\n</context>'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "retrieval_qa_chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7ab1ea90-7c5b-4e4a-9421-4ebb437b4636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context appears to be a collection of text snippets, likely from different documents or files. Here's what I've gathered:\n",
      "\n",
      "1. The first snippet seems to be an error message from Jupyter Notebook, indicating that the kernel crashed while executing code in the current cell or a previous cell.\n",
      "2. The second snippet is a passage about LLaMA v2, which appears to be an open-source large language model developed by Meta AI. It mentions the model's size, performance, and potential applications.\n",
      "3. The third snippet discusses LangChain, an open-source framework for developing large language models (LLMs). It highlights the framework's modular design, core concepts, and benefits.\n",
      "\n",
      "It seems that these snippets might be related to a project or research involving natural language processing, machine learning, or AI. However, I'm not sure what specific task or question is being asked, as there doesn't appear to be a clear query or prompt.\n",
      "\n",
      "If you could provide more context or clarify the question you'd like me to answer, I'll do my best to help!\n"
     ]
    }
   ],
   "source": [
    "chain = retrieval_qa_chat_prompt | llm | StrOutputParser()\n",
    "answer = chain.invoke(\n",
    "    {\n",
    "        \"input\": \"Explain the context given to you!\",\n",
    "        \"context\": vector_map[\"What is RAG\"],\n",
    "    }\n",
    ")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
